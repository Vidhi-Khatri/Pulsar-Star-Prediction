{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                as pd\n",
    "import numpy                 as np\n",
    "import matplotlib.pyplot     as plt\n",
    "import seaborn               as sns\n",
    "from keras.models            import Sequential\n",
    "from keras.layers            import Dense, Dropout\n",
    "from keras.optimizers        import Adam\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics         import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics         import precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from IPython.core.display    import display, HTML\n",
    "from IPython.display         import display_html\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "- [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "    \n",
    "    \n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "    - [Data Manipulation](#Data-Manipulation)\n",
    "    - [Interaction Columns](#Interaction-Columns)\n",
    "    \n",
    "    \n",
    "- [Establishing The Baseline](#Establishing-The-Baseline)\n",
    "    \n",
    "    \n",
    "- [Modeling](#Modeling)\n",
    "    - [Subset Definition](#Subset-Definition)\n",
    "    - [Defining X & y Variables](#Defining-X-&-y-Variables)\n",
    "    - [Train-Test Split](#Train-Test-Split)\n",
    "    - [Evaluation Functions](#Evaluation-Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulsar = pd.read_csv(\"../Data/pulsar_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the head of the data\n",
    "\n",
    "pulsar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the data\n",
    "\n",
    "print(f\"The shape of the dataset is: {pulsar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of column data types\n",
    "\n",
    "pulsar.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "pulsar.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While visualizing the data, we noticed that there are two columns that appeared to have a close to normal distribution.  As a result, we decided to square the values to transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squaring the `mean_ip` column\n",
    "\n",
    "pulsar[\"mean_ip_squared\"] = pulsar[\"mean_ip\"].apply(lambda x: x**2)\n",
    "\n",
    "# Squaring the `sd_ip` column\n",
    "\n",
    "pulsar[\"sd_ip_squared\"]   = pulsar[\"sd_ip\"].apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure new columns were added\n",
    "\n",
    "pulsar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the heat map in the previous notebook, we noticed that there are some columns with very high correlations.  We felt that creating interaction columns, we would be emphasizing the correlation while also reducing the number of features.\n",
    "\n",
    "\n",
    "The columns in particular are:\n",
    "\n",
    "\n",
    "| Column 1   | Column 2   | Correlation |\n",
    "|:-----------|:-----------|:-----------:|\n",
    "| mean_ip    | sd_ip      | 0.55        |\n",
    "| ex_kurt_ip | skew_ip    | 0.95        |\n",
    "| mean_dmsnr | sd_dmsnr   | 0.80        |\n",
    "| ex_kurt_ip | skew_dmsnr | 0.92        |\n",
    "\n",
    "\n",
    "We chose these columns in particular because they allows us to reduce the number of features that will end up going into the model.  However, we decided not to get ride of the eight original features: we will end up with sub-sets of the model we will model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the `mean_ip` * `sd_ip` colum\n",
    "# Defining the `ex_kurt_ip` * `skew_ip` colum\n",
    "\n",
    "pulsar[\"mean_*_sd_ip\"]     = pulsar[\"mean_ip\"] * pulsar[\"sd_ip\"]\n",
    "pulsar[\"exkurt_*_skew_ip\"] = pulsar[\"ex_kurt_ip\"] * pulsar[\"skew_ip\"]\n",
    "\n",
    "# Defining the `mean_dmsnr` * `sd_dmsnr` colum\n",
    "# Defining the `ex_kurt_dmsnr` * `skew_dmsnr` colum\n",
    "\n",
    "pulsar[\"mean_*_sd_dmsnr\"]     = pulsar[\"mean_dmsnr\"] * pulsar[\"sd_dmsnr\"]\n",
    "pulsar[\"exkurt_*_skew_dmsnr\"] = pulsar[\"ex_kurt_dmsnr\"] * pulsar[\"skew_dmsnr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure columns were created\n",
    "\n",
    "pulsar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what our baseline accuracy is because that will give us an accuracy score to beat: if our accuracy is less than the baseline it means that our model is worse than guessing the category of a star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentages of each class\n",
    "\n",
    "round(pulsar[\"target_class\"].value_counts(normalize = True)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of the classes\n",
    "\n",
    "tick_labels = [\"Non-Pulsar\", \"Pulsar\"]\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "# Plotting the graph\n",
    "sns.countplot(pulsar[\"target_class\"])\n",
    "\n",
    "# Setting graph parameters\n",
    "plt.title(\"Pulsar Or Not A Pulsar?\", size = 18)\n",
    "plt.xlabel(\"Star Type\", size = 16)\n",
    "plt.ylabel(\"Number Of Stars\", size = 16)\n",
    "\n",
    "# Making sure the only ticks are 0 and 1\n",
    "plt.xticks(np.arange(0,2,1),\n",
    "           labels = tick_labels,\n",
    "           size   = 14)\n",
    "plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is _extremely_ imbalanced: this will make it difficult to model because the negative class (non-pulsar) is so much less frequent than the positive class (pulsar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that because of the extreme imbalance of the classes a feed forward neural network is the best approach to predicting the presence of a pulsar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way we set up the interaction columns and manipulated the data, we decided that we are best off with three subsets: the original features, original features with squared columns, and the interaction columns.  We have to define the subsets before we define our X and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulsar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the original features\n",
    "\n",
    "original_features = [\"mean_ip\", \"sd_ip\", \"ex_kurt_ip\", \"skew_ip\",\n",
    "                     \"mean_dmsnr\", \"sd_dmsnr\", \"ex_kurt_dmsnr\", \"skew_dmsnr\",\n",
    "                     \"target_class\"]\n",
    "\n",
    "# List of the original features with `mean_ip` and `sd_ip` squared\n",
    "\n",
    "manipulated_features = [\"mean_ip_squared\", \"sd_ip_squared\", \"ex_kurt_ip\", \"skew_ip\",\n",
    "                        \"mean_dmsnr\", \"sd_dmsnr\", \"ex_kurt_dmsnr\", \"skew_dmsnr\",\n",
    "                        \"target_class\"]\n",
    "\n",
    "# List of the interaction features\n",
    "\n",
    "interaction_features = [\"mean_*_sd_ip\", \"exkurt_*_skew_ip\", \"mean_*_sd_dmsnr\",\n",
    "                        \"exkurt_*_skew_dmsnr\", \"target_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a subset with the original features\n",
    "\n",
    "pulsar_og = pulsar[original_features]\n",
    "\n",
    "# Defining a subset with the original/squared features\n",
    "\n",
    "pulsar_sq = pulsar[manipulated_features]\n",
    "\n",
    "# Defining a subset with the interaction features\n",
    "\n",
    "pulsar_if = pulsar[interaction_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining X & y Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on `X` and `y` will refer to the original features, `_sq` will refer to the dataframe with the squared features, and `_if` will refer to the dataframe with interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y for the original\n",
    "\n",
    "X = pulsar_og.drop(\"target_class\", axis = 1)\n",
    "y = pulsar_og[\"target_class\"]\n",
    "\n",
    "# X and y for the original/squared features\n",
    "\n",
    "X_sq = pulsar_sq.drop(\"target_class\", axis = 1)\n",
    "y_sq = pulsar_sq[\"target_class\"]\n",
    "\n",
    "# X and y for the interaction features\n",
    "\n",
    "X_if = pulsar_int.drop(\"target_class\", axis = 1)\n",
    "y_if = pulsar_int[\"target_class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the train-test split is to split up our data so that we can reserve an unseen portion of it to test our model on.  Additionally, we will be setting a random state for reproducability and we will stratify on `y` so that the distribution of classes is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the original data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)\n",
    "\n",
    "# For the original data with squared features\n",
    "\n",
    "X_sq_train, X_sq_test, y_sq_train, y_sq_test = train_test_split(X_sq,\n",
    "                                                                y_sq,\n",
    "                                                                random_state = 42,\n",
    "                                                                stratify     = y_sq )\n",
    "\n",
    "# For the dataframe with interaction features\n",
    "\n",
    "X_if_train, X_if_test, y_if_train, y_if_test = train_test_split(X_if,\n",
    "                                                                y_if,\n",
    "                                                                random_state = 42,\n",
    "                                                                stratify     = y_if)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix gives us a summary of how our model classified the test data: it compares the true and predicted y values so that we can see how the model performed on each class.\n",
    "\n",
    "\n",
    "Each confusion matrix is set up the same way:\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|:--------------------|:------------------:|:------------------:|\n",
    "| **Actual Positive** | True Positive      | False Negative     |\n",
    "| **Actual Negative** | False Positive     | True Negative      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We converted the confusion matrix to a dataframe to make it easier to read\n",
    "\n",
    "def create_confusion_matrix(y, y_preds):\n",
    "    cm     = confusion_matrix(y, y_preds)\n",
    "    matrix = pd.DataFrame(cm, \n",
    "                          columns = [\"Predicted r/Cooking\", \"Predicted r/AskCulinary\"], \n",
    "                          index   = [\"Actual r/Cooking\", \"Actual r/AskCulinary\"])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate two scores based off of the confusion matrix: specificity and sensitivity; we will go into what these scores measure in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating specificity from a confusion matrix\n",
    "\n",
    "def specificity(y, y_pred):\n",
    "    cm          = confusion_matrix(y, y_pred)  \n",
    "    specificity = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    return specificity\n",
    "\n",
    "# Calculating sensitivity from a confusion matrix\n",
    "\n",
    "def sensitivity(y, y_pred):\n",
    "    cm          = confusion_matrix(y, y_pred)\n",
    "    sensitivity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to metric scores, we will also calculate an ROC-AUC score.  The ROC (receiver operating characteristic) shows us a binary classification model's ability to distinguish between two classes; we will plot this curve for the best model as determined by metric scores.\n",
    "\n",
    "These images from [GreyAtom](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) illustrates the AUC-ROC well:\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.8 0.9.png\" alt = \"high auc_roc scores\" height = \"350\" width = \"350\">\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.5 0.7.png\" alt = \"low auc_roc scores\" height = \"350\" width = \"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
