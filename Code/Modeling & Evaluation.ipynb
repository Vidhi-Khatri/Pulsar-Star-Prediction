{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                as pd\n",
    "import numpy                 as np\n",
    "import matplotlib.pyplot     as plt\n",
    "import seaborn               as sns\n",
    "from keras                   import regularizers\n",
    "from keras.models            import Sequential\n",
    "from keras.layers            import Dense, Dropout\n",
    "from keras.optimizers        import Adam\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics         import roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.metrics         import f1_score\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from IPython.core.display    import display, HTML\n",
    "from IPython.display         import display_html\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "- [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "    \n",
    "\n",
    "- [Establishing The Baseline](#Establishing-The-Baseline)\n",
    "    \n",
    "    \n",
    "- [Preprocessing](#Preprocessing)\n",
    "    - [Feature Engineering](#Feature-Engineering)\n",
    "        - [Data Manipulation](#Data-Manipulation)\n",
    "        - [Interaction Columns](#Interaction-Columns)\n",
    "    - [Subset Definition](#Subset-Definition)\n",
    "    - [Defining X & y Variables](#Defining-X-&-y-Variables)\n",
    "    - [Train-Test Split](#Train-Test-Split)\n",
    "    - [Scaling The Data](#Scaling-The-Data)\n",
    "\n",
    "  \n",
    "- [Modeling](#Modeling)\n",
    "    - [Evaluation Functions](#Evaluation-Functions)\n",
    "    - [Neural Networks](#Neural-Networks)\n",
    "        - [Original Features](#Original-Features)\n",
    "        - [Squared Features](#Squared-Features)\n",
    "        - [Interaction Features](#Interaction-Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulsar = pd.read_csv(\"../Data/pulsar_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the head of the data\n",
    "\n",
    "pulsar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the data\n",
    "\n",
    "print(f\"The shape of the dataset is: {pulsar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of column data types\n",
    "\n",
    "pulsar.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "pulsar.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what our baseline accuracy is because that will give us an accuracy score to beat: if our accuracy is less than the baseline it means that our model is worse than guessing the category of a star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentages of each class\n",
    "\n",
    "round(pulsar[\"target_class\"].value_counts(normalize = True)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of the classes\n",
    "\n",
    "tick_labels = [\"Non-Pulsar\", \"Pulsar\"]\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "# Plotting the graph\n",
    "sns.countplot(pulsar[\"target_class\"])\n",
    "\n",
    "# Setting graph parameters\n",
    "plt.title(\"Star Types\", size = 18)\n",
    "plt.xlabel(\"Star Type\", size = 16)\n",
    "plt.ylabel(\"Number Of Stars\", size = 16)\n",
    "\n",
    "# Making sure the only ticks are 0 and 1\n",
    "plt.xticks(np.arange(0,2,1),\n",
    "           labels = tick_labels,\n",
    "           size   = 14)\n",
    "plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is _extremely_ imbalanced: this will make it difficult to model because the negative class (non-pulsar) is so much less frequent than the positive class (pulsar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While visualizing the data, we noticed that there are two columns that appeared to have a close to normal distribution.  As a result, we decided to square the values to transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squaring the `mean_ip` column\n",
    "\n",
    "pulsar[\"mean_ip_squared\"] = pulsar[\"mean_ip\"].apply(lambda x: x**2)\n",
    "\n",
    "# Squaring the `sd_ip` column\n",
    "\n",
    "pulsar[\"sd_ip_squared\"]   = pulsar[\"sd_ip\"].apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure new columns were added\n",
    "\n",
    "pulsar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the heat map in the previous notebook, we noticed that there are some columns with very high correlations.  We felt that creating interaction columns, we would be emphasizing the correlation while also reducing the number of features.\n",
    "\n",
    "\n",
    "The columns in particular are:\n",
    "\n",
    "\n",
    "| Column 1   | Column 2   | Correlation |\n",
    "|:-----------|:-----------|:-----------:|\n",
    "| mean_ip    | sd_ip      | 0.55        |\n",
    "| ex_kurt_ip | skew_ip    | 0.95        |\n",
    "| mean_dmsnr | sd_dmsnr   | 0.80        |\n",
    "| ex_kurt_ip | skew_dmsnr | 0.92        |\n",
    "\n",
    "\n",
    "We chose these columns in particular because they allows us to reduce the number of features that will end up going into the model.  However, we decided not to get ride of the eight original features: we will end up with sub-sets of the model we will model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the `mean_ip` * `sd_ip` colum\n",
    "# Defining the `ex_kurt_ip` * `skew_ip` colum\n",
    "\n",
    "pulsar[\"mean_*_sd_ip\"]     = pulsar[\"mean_ip\"] * pulsar[\"sd_ip\"]\n",
    "pulsar[\"exkurt_*_skew_ip\"] = pulsar[\"ex_kurt_ip\"] * pulsar[\"skew_ip\"]\n",
    "\n",
    "# Defining the `mean_dmsnr` * `sd_dmsnr` colum\n",
    "# Defining the `ex_kurt_dmsnr` * `skew_dmsnr` colum\n",
    "\n",
    "pulsar[\"mean_*_sd_dmsnr\"]     = pulsar[\"mean_dmsnr\"] * pulsar[\"sd_dmsnr\"]\n",
    "pulsar[\"exkurt_*_skew_dmsnr\"] = pulsar[\"ex_kurt_dmsnr\"] * pulsar[\"skew_dmsnr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure columns were created\n",
    "\n",
    "pulsar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that because of the extreme imbalance of the classes a feed forward neural network is the best approach to predicting the presence of a pulsar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way we set up the interaction columns and manipulated the data, we decided that we are best off with three subsets: the original features, original features with squared columns, and the interaction columns.  We have to define the subsets before we define our X and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the original features\n",
    "\n",
    "original_features = [\"mean_ip\", \"sd_ip\", \"ex_kurt_ip\", \"skew_ip\",\n",
    "                     \"mean_dmsnr\", \"sd_dmsnr\", \"ex_kurt_dmsnr\", \"skew_dmsnr\",\n",
    "                     \"target_class\"]\n",
    "\n",
    "# List of the original features with `mean_ip` and `sd_ip` squared\n",
    "\n",
    "manipulated_features = [\"mean_ip_squared\", \"sd_ip_squared\", \"ex_kurt_ip\", \"skew_ip\",\n",
    "                        \"mean_dmsnr\", \"sd_dmsnr\", \"ex_kurt_dmsnr\", \"skew_dmsnr\",\n",
    "                        \"target_class\"]\n",
    "\n",
    "# List of the interaction features\n",
    "\n",
    "interaction_features = [\"mean_*_sd_ip\", \"exkurt_*_skew_ip\", \"mean_*_sd_dmsnr\",\n",
    "                        \"exkurt_*_skew_dmsnr\", \"target_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a subset with the original features\n",
    "\n",
    "pulsar_og = pulsar[original_features]\n",
    "\n",
    "# Defining a subset with the original/squared features\n",
    "\n",
    "pulsar_sq = pulsar[manipulated_features]\n",
    "\n",
    "# Defining a subset with the interaction features\n",
    "\n",
    "pulsar_if = pulsar[interaction_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining X & y Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on `X` and `y` will refer to the original features, `_sq` will refer to the dataframe with the squared features, and `_if` will refer to the dataframe with interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y for the original\n",
    "\n",
    "X = pulsar_og.drop(\"target_class\", axis = 1)\n",
    "y = pulsar_og[\"target_class\"]\n",
    "\n",
    "# X and y for the original/squared features\n",
    "\n",
    "X_sq = pulsar_sq.drop(\"target_class\", axis = 1)\n",
    "y_sq = pulsar_sq[\"target_class\"]\n",
    "\n",
    "# X and y for the interaction features\n",
    "\n",
    "X_if = pulsar_if.drop(\"target_class\", axis = 1)\n",
    "y_if = pulsar_if[\"target_class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the train-test split is to split up our data so that we can reserve an unseen portion of it to test our model on.  Additionally, we will be setting a random state for reproducability and we will stratify on `y` so that the distribution of classes is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the original data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)\n",
    "\n",
    "# For the original data with squared features\n",
    "\n",
    "X_sq_train, X_sq_test, y_sq_train, y_sq_test = train_test_split(X_sq,\n",
    "                                                                y_sq,\n",
    "                                                                random_state = 42,\n",
    "                                                                stratify     = y_sq )\n",
    "\n",
    "# For the dataframe with interaction features\n",
    "\n",
    "X_if_train, X_if_test, y_if_train, y_if_test = train_test_split(X_if,\n",
    "                                                                y_if,\n",
    "                                                                random_state = 42,\n",
    "                                                                stratify     = y_if)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix gives us a summary of how our model classified the test data: it compares the true and predicted y values so that we can see how the model performed on each class.\n",
    "\n",
    "\n",
    "Each confusion matrix is set up the same way:\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|:--------------------|:------------------:|:------------------:|\n",
    "| **Actual Positive** | True Positive      | False Negative     |\n",
    "| **Actual Negative** | False Positive     | True Negative      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We converted the confusion matrix to a dataframe to make it easier to read\n",
    "\n",
    "def create_confusion_matrix(y, y_preds):\n",
    "    cm     = confusion_matrix(y, y_preds)\n",
    "    matrix = pd.DataFrame(cm, \n",
    "                          columns = [\"Predicted Pulsar\", \"Predicted Non-Pulsar\"], \n",
    "                          index   = [\"Actual Pulsar\", \"Actual Non-Pulsar\"])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate two scores based off of the confusion matrix: specificity and sensitivity; we will go into what these scores measure in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating specificity from a confusion matrix\n",
    "\n",
    "def specificity(y, y_pred):\n",
    "    cm          = confusion_matrix(y, y_pred)  \n",
    "    specificity = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    return specificity\n",
    "\n",
    "# Calculating sensitivity from a confusion matrix\n",
    "\n",
    "def sensitivity(y, y_pred):\n",
    "    cm          = confusion_matrix(y, y_pred)\n",
    "    sensitivity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to metric scores, we will also calculate an ROC-AUC score.  The ROC (receiver operating characteristic) shows us a binary classification model's ability to distinguish between two classes; we will plot this curve for the best model as determined by metric scores.\n",
    "\n",
    "These images from [GreyAtom](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) illustrates the AUC-ROC well:\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.8 0.9.png\" alt = \"high auc_roc scores\" height = \"350\" width = \"350\">\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.5 0.7.png\" alt = \"low auc_roc scores\"  height = \"350\" width = \"350\">\n",
    "\n",
    "\n",
    "Accuracy is not the most informative score for us: it just counts how many posts were classified correctly.  Instead, we want to look at performance regarding the predicted positives (pulsars) and negatives (non-pulsars).  We will use the following scores:\n",
    "\n",
    "| Metric                | Definition                                                       | Scale    |\n",
    "|:----------------------|:-----------------------------------------------------------------|:---------|\n",
    "| **Balanced Accuracy** | The average of the recall on each class                          | 0 to 1   | \n",
    "| **Specificity**       | How many negative predictions are correct                        | 0 to 1   | \n",
    "| **Sensitivity**       | How many positive predictions are correct (also known as recall) | 0 to 1   | \n",
    "| **F1 Score**          | Accuracy that takes into account the specificity & sensitivity   | 0 to 1   | \n",
    "| **ROC-AUC Score**     | A measure of the model's ability to distinguish classes          | 0.5 to 1 |\n",
    "\n",
    "One of the parameters for the F1 and ROC-AUC scores is called `average` which, as the name suggests, controls how the averages are calculate.  The default is `binary`, but because our classes are extremely unbalanced we will use the value `macro` which calculates the score on each class but does _not_ apply any kind of weighting; doing this ignores the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the 6 metric evaluation\n",
    "\n",
    "def generate_model_eval(y, y_pred):\n",
    "    print(f\"The balanced accuracy score is: {round(balanced_accuracy_score(y, y_pred), 5)}\")\n",
    "    print(f\"The specificity score is      : {round(specificity(y, y_pred), 5)}\")\n",
    "    print(f\"The sensitivity score is      : {round(sensitivity(y, y_pred), 5)}\")\n",
    "    print(f\"The F1 score is               : {round(f1_score(y, y_pred, average = 'macro'), 5)}\")\n",
    "    print(f\"The ROC-AUC score is          : {round(roc_auc_score(y, y_pred, average = 'macro'), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/neural_network.jpg\" alt = \"simple neural network\" height = \"350\" width = \"400\">\n",
    "\n",
    "\n",
    "A neural network is a modeling technique that consists of a number of simple but highly interconnected elements or nodes or neurons \n",
    "which are organized in layers.\n",
    "\n",
    "\n",
    "- **Input layer**: the number of features in the data; in our case, the number of columns\n",
    "\n",
    "\n",
    "- **Hidden layer(s)**: the \"middle\" features and are called hidden because they are neither the input nor output.  The hidden layer(s) are where the actual computations take place: using weights and biases they modify linear models.  The hidden layers also have activation functions which modify the output in some way; the most common activation function is ReLU (rectified linear unit) which forces the hidden layer to be positive.\n",
    "\n",
    "\n",
    "- **Output layer**: what the actual result of the computation is and they determine what the results look like.  In the case of regression, it is a number whereas in classification the output is a probability.\n",
    "\n",
    "\n",
    "If we are not careful, our neural networks can become _extremely_ complicated and overfit to the training data.  To combat that, we will regularize the models.\n",
    "\n",
    "\n",
    "The models will have the following structure:\n",
    "\n",
    "- A dense network\n",
    "\n",
    "- An input layer\n",
    "\n",
    "- Hidden layers\n",
    "    - ReLU activation\n",
    "\n",
    "- Single output layer\n",
    "    - Sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the input size\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Instantiating the model\n",
    "\n",
    "of_model = Sequential()\n",
    "\n",
    "# Adding a primary dense layer\n",
    "\n",
    "of_model.add(Dense(6,\n",
    "                   input_dim          = input_shape,\n",
    "                   activation         = \"relu\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Adding an output layer\n",
    "\n",
    "of_model.add(Dense(1,\n",
    "                   activation         = \"sigmoid\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Compiling the model\n",
    "\n",
    "of_model.compile(loss      = \"binary_crossentropy\",\n",
    "                 optimizer = \"adam\",\n",
    "                 metrics   = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "\n",
    "of_model.fit(X_train,\n",
    "             y_train,\n",
    "             epochs     = 150,\n",
    "             batch_size = 100,\n",
    "             verbose    = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "of_train_preds = of_model.predict_classes(X_train,\n",
    "                                          batch_size = 100,\n",
    "                                          verbose    = 0).ravel()\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "of_test_preds = of_model.predict_classes(X_test,\n",
    "                                         batch_size = 100,\n",
    "                                         verbose    = 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the training predictions\n",
    "\n",
    "generate_model_eval(y_train, of_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test predictions\n",
    "\n",
    "generate_model_eval(y_test, of_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(y_test, of_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, of_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
