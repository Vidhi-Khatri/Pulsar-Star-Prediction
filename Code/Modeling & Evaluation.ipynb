{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                as pd\n",
    "import numpy                 as np\n",
    "import matplotlib.pyplot     as plt\n",
    "import seaborn               as sns\n",
    "from keras                   import regularizers\n",
    "from keras.models            import Sequential\n",
    "from keras.layers            import Dense\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics         import roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.metrics         import f1_score, confusion_matrix, recall_score\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.preprocessing   import StandardScaler, LabelBinarizer\n",
    "from IPython.core.display    import display, HTML\n",
    "from IPython.display         import display_html\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "- [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "    \n",
    "\n",
    "- [Establishing The Baseline](#Establishing-The-Baseline)\n",
    "    \n",
    "    \n",
    "- [Preprocessing](#Preprocessing)\n",
    "    - [Feature Engineering](#Feature-Engineering)\n",
    "        - [Data Manipulation](#Data-Manipulation)\n",
    "        - [Interaction Columns](#Interaction-Columns)\n",
    "    - [Subset Definition](#Subset-Definition)\n",
    "    - [Defining X & y Variables](#Defining-X-&-y-Variables)\n",
    "    - [Train-Test Split](#Train-Test-Split)\n",
    "    - [Scaling The Data](#Scaling-The-Data)\n",
    "\n",
    "  \n",
    "- [Modeling](#Modeling)\n",
    "    - [Evaluation Functions](#Evaluation-Functions)\n",
    "    - [Neural Networks](#Neural-Networks)\n",
    "        - [Original Features](#Original-Features)\n",
    "        - [Squared Features](#Squared-Features)\n",
    "        - [Interaction Features](#Interaction-Features)\n",
    "        \n",
    "        \n",
    "- [Evaluation](#Evaluation)\n",
    "    - [Functions](#Functions)\n",
    "    - [Dataframes](#Dataframes)\n",
    "    - [Best Model Selection](#Best-Model-Selection)\n",
    "    - [Plots](#Plots)\n",
    "        - [Bar Chart](#Bar-Chart)\n",
    "        - [ROC Curve](#ROC-Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulsar = pd.read_csv(\"../Data/pulsar_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the head of the data\n",
    "\n",
    "pulsar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the data\n",
    "\n",
    "print(f\"The shape of the dataset is: {pulsar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of column data types\n",
    "\n",
    "pulsar.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "pulsar.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what our baseline accuracy is because that will give us an accuracy score to beat: if our accuracy is less than the baseline it means that our model is worse than guessing the category of a star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentages of each class\n",
    "\n",
    "round(pulsar[\"target_class\"].value_counts(normalize = True)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of the classes\n",
    "\n",
    "tick_labels = [\"Non-Pulsar\", \"Pulsar\"]\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "# Plotting the graph\n",
    "sns.countplot(pulsar[\"target_class\"])\n",
    "\n",
    "# Setting graph parameters\n",
    "plt.title(\"Star Types\", size = 18)\n",
    "plt.xlabel(\"Star Type\", size = 16)\n",
    "plt.ylabel(\"Number Of Stars\", size = 16)\n",
    "\n",
    "# Making sure the only ticks are 0 and 1\n",
    "plt.xticks(np.arange(0,2,1),\n",
    "           labels = tick_labels,\n",
    "           size   = 14)\n",
    "plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is _extremely_ imbalanced: this will make it difficult to model because the negative class (non-pulsar) is so much less frequent than the positive class (pulsar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While visualizing the data, we noticed that there are two columns that appeared to have a close to normal distribution.  As a result, we decided to square the values to transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squaring the `mean_ip` column\n",
    "\n",
    "pulsar[\"mean_ip_squared\"] = pulsar[\"mean_ip\"].apply(lambda x: x**2)\n",
    "\n",
    "# Squaring the `sd_ip` column\n",
    "\n",
    "pulsar[\"sd_ip_squared\"]   = pulsar[\"sd_ip\"].apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure new columns were added\n",
    "\n",
    "pulsar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the heat map in the previous notebook, we noticed that there are some columns with very high correlations.  We felt that creating interaction columns, we would be emphasizing the correlation while also reducing the number of features.\n",
    "\n",
    "\n",
    "The columns in particular are:\n",
    "\n",
    "\n",
    "| Column 1   | Column 2   | Correlation |\n",
    "|:-----------|:-----------|:-----------:|\n",
    "| mean_ip    | sd_ip      | 0.55        |\n",
    "| ex_kurt_ip | skew_ip    | 0.95        |\n",
    "| mean_dmsnr | sd_dmsnr   | 0.80        |\n",
    "| ex_kurt_ip | skew_dmsnr | 0.92        |\n",
    "\n",
    "\n",
    "We chose these columns in particular because they allows us to reduce the number of features that will end up going into the model.  However, we decided not to get ride of the eight original features: we will end up with sub-sets of the model we will model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the `mean_ip` * `sd_ip` colum\n",
    "# Defining the `ex_kurt_ip` * `skew_ip` colum\n",
    "\n",
    "pulsar[\"mean_*_sd_ip\"]     = pulsar[\"mean_ip\"] * pulsar[\"sd_ip\"]\n",
    "pulsar[\"exkurt_*_skew_ip\"] = pulsar[\"ex_kurt_ip\"] * pulsar[\"skew_ip\"]\n",
    "\n",
    "# Defining the `mean_dmsnr` * `sd_dmsnr` colum\n",
    "# Defining the `ex_kurt_dmsnr` * `skew_dmsnr` colum\n",
    "\n",
    "pulsar[\"mean_*_sd_dmsnr\"]     = pulsar[\"mean_dmsnr\"] * pulsar[\"sd_dmsnr\"]\n",
    "pulsar[\"exkurt_*_skew_dmsnr\"] = pulsar[\"ex_kurt_dmsnr\"] * pulsar[\"skew_dmsnr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure columns were created\n",
    "\n",
    "pulsar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that because of the extreme imbalance of the classes a feed forward neural network is the best approach to predicting the presence of a pulsar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way we set up the interaction columns and manipulated the data, we decided that we are best off with three subsets: the original features, original features with squared columns, and the interaction columns.  We have to define the subsets before we define our X and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the original features\n",
    "\n",
    "original_features = [\"mean_ip\", \"sd_ip\", \"ex_kurt_ip\", \"skew_ip\",\n",
    "                     \"mean_dmsnr\", \"sd_dmsnr\", \"ex_kurt_dmsnr\", \"skew_dmsnr\",\n",
    "                     \"target_class\"]\n",
    "\n",
    "# List of the original features with `mean_ip` and `sd_ip` squared\n",
    "\n",
    "manipulated_features = [\"mean_ip_squared\", \"sd_ip_squared\", \"ex_kurt_ip\", \"skew_ip\",\n",
    "                        \"mean_dmsnr\", \"sd_dmsnr\", \"ex_kurt_dmsnr\", \"skew_dmsnr\",\n",
    "                        \"target_class\"]\n",
    "\n",
    "# List of the interaction features\n",
    "\n",
    "interaction_features = [\"mean_*_sd_ip\", \"exkurt_*_skew_ip\", \"mean_*_sd_dmsnr\",\n",
    "                        \"exkurt_*_skew_dmsnr\", \"target_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a subset with the original features\n",
    "\n",
    "pulsar_og = pulsar[original_features]\n",
    "\n",
    "# Defining a subset with the original/squared features\n",
    "\n",
    "pulsar_sq = pulsar[manipulated_features]\n",
    "\n",
    "# Defining a subset with the interaction features\n",
    "\n",
    "pulsar_if = pulsar[interaction_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining X & y Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on `X` and `y` will refer to the original features, `_sq` will refer to the dataframe with the squared features, and `_if` will refer to the dataframe with interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y for the original\n",
    "\n",
    "X = pulsar_og.drop(\"target_class\", axis = 1)\n",
    "y = pulsar_og[\"target_class\"]\n",
    "\n",
    "# X and y for the original/squared features\n",
    "\n",
    "X_sq = pulsar_sq.drop(\"target_class\", axis = 1)\n",
    "y_sq = pulsar_sq[\"target_class\"]\n",
    "\n",
    "# X and y for the interaction features\n",
    "\n",
    "X_if = pulsar_if.drop(\"target_class\", axis = 1)\n",
    "y_if = pulsar_if[\"target_class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the train-test split is to split up our data so that we can reserve an unseen portion of it to test our model on.  Additionally, we will be setting a random state for reproducability and we will stratify on `y` so that the distribution of classes is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the scaler\n",
    "\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming here makes it easier than\n",
    "# scaling after the train-test split\n",
    "\n",
    "X_ss = ss.fit_transform(X)\n",
    "X_sq_ss = ss.fit_transform(X_sq)\n",
    "X_if_ss = ss.fit_transform(X_if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the original data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ss,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)\n",
    "\n",
    "# For the original data with squared features\n",
    "\n",
    "X_sq_train, X_sq_test, y_sq_train, y_sq_test = train_test_split(X_sq_ss,\n",
    "                                                                y_sq,\n",
    "                                                                random_state = 42,\n",
    "                                                                stratify     = y_sq )\n",
    "\n",
    "# For the dataframe with interaction features\n",
    "\n",
    "X_if_train, X_if_test, y_if_train, y_if_test = train_test_split(X_if_ss,\n",
    "                                                                y_if,\n",
    "                                                                random_state = 42,\n",
    "                                                                stratify     = y_if)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix gives us a summary of how our model classified the test data: it compares the true and predicted y values so that we can see how the model performed on each class.\n",
    "\n",
    "\n",
    "Each confusion matrix is set up the same way:\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|:--------------------|:------------------:|:------------------:|\n",
    "| **Actual Positive** | True Negative      | False Negative     |\n",
    "| **Actual Negative** | False Positive     | True Positive      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We converted the confusion matrix to a dataframe to make it easier to read\n",
    "\n",
    "def create_confusion_matrix(y, y_preds):\n",
    "    cm     = confusion_matrix(y, y_preds)\n",
    "    matrix = pd.DataFrame(cm, \n",
    "                          columns = [\"Predicted Non-Pulsar\", \"Predicted Pulsar\"], \n",
    "                          index   = [\"Actual Non-Pulsar\", \"Predicted Pulsar\"])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate two scores based off of the confusion matrix: specificity and sensitivity; we will go into what these scores measure in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating specificity from a confusion matrix\n",
    "\n",
    "def specificity(y, y_pred):\n",
    "    cm          = confusion_matrix(y, y_pred)  \n",
    "    specificity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to metric scores, we will also calculate an ROC-AUC score.  The ROC (receiver operating characteristic) shows us a binary classification model's ability to distinguish between two classes; we will plot this curve for the best model as determined by metric scores.\n",
    "\n",
    "These images from [GreyAtom](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) illustrates the AUC-ROC well:\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.8 0.9.png\" alt = \"high auc_roc scores\" height = \"350\" width = \"350\">\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.5 0.7.png\" alt = \"low auc_roc scores\"  height = \"350\" width = \"350\">\n",
    "\n",
    "\n",
    "Accuracy is not the most informative score for us: it just counts how many posts were classified correctly.  Instead, we want to look at performance regarding the predicted positives (pulsars) and negatives (non-pulsars).  We will use the following scores:\n",
    "\n",
    "| Metric                | Definition                                                       | Scale    |\n",
    "|:----------------------|:-----------------------------------------------------------------|:---------|\n",
    "| **Balanced Accuracy** | The average of the recall on each class                          | 0 to 1   | \n",
    "| **Specificity**       | How many negative predictions are correct                        | 0 to 1   | \n",
    "| **Sensitivity**       | How many positive predictions are correct (also known as recall) | 0 to 1   | \n",
    "| **F1 Score**          | Accuracy that takes into account the specificity & sensitivity   | 0 to 1   | \n",
    "| **ROC-AUC Score**     | A measure of the model's ability to distinguish classes          | 0.5 to 1 |\n",
    "\n",
    "One of the parameters for the F1 and ROC-AUC scores is called `average` which, as the name suggests, controls how the averages are calculate.  The default is `binary`, but because our classes are extremely unbalanced we will use the value `macro` which calculates the score on each class but does _not_ apply any kind of weighting; doing this ignores the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the 6 metric evaluation\n",
    "\n",
    "def generate_model_eval(y, y_pred):\n",
    "    print(f\"The balanced accuracy score is: {round(balanced_accuracy_score(y, y_pred), 5)}\")\n",
    "    print(f\"The specificity score is      : {round(specificity(y, y_pred), 5)}\")\n",
    "    print(f\"The sensitivity score is      : {round(recall_score(y, y_pred), 5)}\")\n",
    "    print(f\"The F1 score is               : {round(f1_score(y, y_pred, average = 'macro'), 5)}\")\n",
    "    print(f\"The ROC-AUC score is          : {round(roc_auc_score(y, y_pred, average = 'macro'), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/neural_network.jpg\" alt = \"simple neural network\" height = \"350\" width = \"400\">\n",
    "\n",
    "\n",
    "A neural network is a modeling technique that consists of a number of simple but highly interconnected elements or nodes or neurons \n",
    "which are organized in layers.\n",
    "\n",
    "\n",
    "- **Input layer**: the number of features in the data; in our case, the number of columns\n",
    "\n",
    "\n",
    "- **Hidden layer(s)**: the \"middle\" features and are called hidden because they are neither the input nor output.  The hidden layer(s) are where the actual computations take place: using weights and biases they modify linear models.  The hidden layers also have activation functions which modify the output in some way; the most common activation function is ReLU (rectified linear unit) which forces the hidden layer to be positive.\n",
    "\n",
    "\n",
    "- **Output layer**: what the actual result of the computation is and they determine what the results look like.  In the case of regression, it is a number whereas in classification the output is a probability.\n",
    "\n",
    "\n",
    "If we are not careful, our neural networks can become _extremely_ complicated and overfit to the training data.  To combat that, we will regularize the models.\n",
    "\n",
    "\n",
    "The models will have the following structure:\n",
    "\n",
    "- A dense network\n",
    "\n",
    "- An input layer\n",
    "\n",
    "- Hidden layers\n",
    "    - ReLU activation\n",
    "\n",
    "- Single output layer\n",
    "    - Sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the input size\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Instantiating the model\n",
    "\n",
    "of_model = Sequential()\n",
    "\n",
    "# Adding a primary dense layer\n",
    "# L1 regularization is not really used with NNs\n",
    "\n",
    "of_model.add(Dense(6,\n",
    "                   input_dim          = input_shape,\n",
    "                   activation         = \"relu\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Adding an output layer\n",
    "# L1 regularization is not really used with NNs\n",
    "\n",
    "of_model.add(Dense(1,\n",
    "                   activation         = \"sigmoid\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Compiling the model\n",
    "\n",
    "of_model.compile(loss      = \"binary_crossentropy\",\n",
    "                 optimizer = \"adam\",\n",
    "                 metrics   = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "\n",
    "of_model.fit(X_train,\n",
    "             y_train,\n",
    "             epochs     = 150,\n",
    "             batch_size = 100,\n",
    "             verbose    = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "of_train_preds = of_model.predict_classes(X_train,\n",
    "                                          batch_size = 100,\n",
    "                                          verbose    = 0).ravel()\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "of_test_preds = of_model.predict_classes(X_test,\n",
    "                                         batch_size = 100,\n",
    "                                         verbose    = 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the training predictions\n",
    "\n",
    "generate_model_eval(y_train, of_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test predictions\n",
    "\n",
    "generate_model_eval(y_test, of_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(y_test, of_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squared Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the input size\n",
    "\n",
    "input_shape = X_sq_train.shape[1]\n",
    "\n",
    "# Instantiating the model\n",
    "\n",
    "sq_model = Sequential()\n",
    "\n",
    "# Adding a primary dense layer\n",
    "# L1 regularization is not really used with NNs\n",
    "\n",
    "sq_model.add(Dense(6,\n",
    "                   input_dim          = input_shape,\n",
    "                   activation         = \"relu\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Adding an output layer\n",
    "# L1 regularization is not really used with NNs\n",
    "\n",
    "sq_model.add(Dense(1,\n",
    "                   activation         = \"sigmoid\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Compiling the model\n",
    "\n",
    "sq_model.compile(loss      = \"binary_crossentropy\",\n",
    "                 optimizer = \"adam\",\n",
    "                 metrics   = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "\n",
    "sq_model.fit(X_sq_train,\n",
    "             y_sq_train,\n",
    "             epochs     = 150,\n",
    "             batch_size = 100,\n",
    "             verbose    = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "sq_train_preds = sq_model.predict_classes(X_sq_train,\n",
    "                                          batch_size = 100,\n",
    "                                          verbose    = 0).ravel()\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "sq_test_preds = sq_model.predict_classes(X_sq_test,\n",
    "                                         batch_size = 100,\n",
    "                                         verbose    = 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the training predictions\n",
    "\n",
    "generate_model_eval(y_sq_train, sq_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test predictions\n",
    "\n",
    "generate_model_eval(y_sq_test, sq_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(y_sq_test, sq_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the input size\n",
    "\n",
    "input_shape = X_if_train.shape[1]\n",
    "\n",
    "# Instantiating the model\n",
    "\n",
    "if_model = Sequential()\n",
    "\n",
    "# Adding a primary dense layer\n",
    "# L1 regularization is not really used with NNs\n",
    "\n",
    "if_model.add(Dense(6,\n",
    "                   input_dim          = input_shape,\n",
    "                   activation         = \"relu\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Adding an output layer\n",
    "# L1 regularization is not really used with NNs\n",
    "\n",
    "if_model.add(Dense(1,\n",
    "                   activation         = \"sigmoid\",\n",
    "                   kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "# Compiling the model\n",
    "\n",
    "if_model.compile(loss      = \"binary_crossentropy\",\n",
    "                 optimizer = \"adam\",\n",
    "                 metrics   = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "\n",
    "if_model.fit(X_if_train,\n",
    "             y_if_train,\n",
    "             epochs     = 150,\n",
    "             batch_size = 100,\n",
    "             verbose    = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "if_train_preds = if_model.predict_classes(X_if_train,\n",
    "                                          batch_size = 100,\n",
    "                                          verbose    = 0).ravel()\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "if_test_preds = if_model.predict_classes(X_if_test,\n",
    "                                         batch_size = 100,\n",
    "                                         verbose    = 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the training predictions\n",
    "\n",
    "generate_model_eval(y_if_train, if_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test predictions\n",
    "\n",
    "generate_model_eval(y_if_test, if_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(y_if_test, if_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar plot of a model's scores\n",
    "\n",
    "def plot_scores(df, column, label):\n",
    "    \n",
    "    # Setting the figure size\n",
    "    plt.figure(figsize   = (15,5),\n",
    "               facecolor = \"white\")\n",
    "    \n",
    "    # Plotting the bar plot\n",
    "    sns.barplot(x    = df.index,\n",
    "                y    = column,\n",
    "                data = df)\n",
    "    \n",
    "    # Setting the baseline line\n",
    "    plt.axhline(9.16, \n",
    "                color = \"black\")\n",
    "    \n",
    "    # Setting graph parameters\n",
    "    plt.title(label, size = 20)\n",
    "    plt.xlabel(\"Model\", size = 18)\n",
    "    plt.ylabel(\"Score\", size = 18)\n",
    "    plt.xticks(size  = 14)\n",
    "    plt.yticks(ticks = np.arange(0,110,10), \n",
    "               size  = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating scores and plotting the ROC curve\n",
    "\n",
    "def roc(model_prob, X_test, y_test, y_pred, title):\n",
    "    \n",
    "    # Calculating probabilities\n",
    "    model_prob    = [i[0] for i in model_prob.predict_proba(X_test)]\n",
    "    \n",
    "    # Creating a dataframeout of the true values & probas\n",
    "    model_pred_df = pd.DataFrame({\"true_values\": y_test,\n",
    "                                  \"pred_probs\" : model_prob})\n",
    "\n",
    "    # Setting threshold values    \n",
    "    thresholds = np.linspace(0, 1, 500) \n",
    "    \n",
    "    # Calculating the sensitivity\n",
    "    def true_positive_rate(df, true_col, pred_prob_col, threshold):\n",
    "        true_positive  = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "        false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "        return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    # Calculating the false positives\n",
    "    def false_positive_rate(df, true_col, pred_prob_col, threshold):\n",
    "        true_negative  = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "        false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "        return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "    # Calculating the sensitivity and false positives for each point in the threhold\n",
    "    tpr_values = [true_positive_rate(model_pred_df, \n",
    "                                     \"true_values\", \n",
    "                                     \"pred_probs\", \n",
    "                                     prob) for prob in thresholds]\n",
    "    fpr_values = [false_positive_rate(model_pred_df, \n",
    "                                      \"true_values\", \n",
    "                                      \"pred_probs\", \n",
    "                                      prob) for prob in thresholds]\n",
    "\n",
    "    # Setting up the graph\n",
    "    plt.figure(figsize   = (13,7),\n",
    "               facecolor = \"white\")\n",
    "    \n",
    "    # Plotting the predicted\n",
    "    plt.plot(fpr_values, \n",
    "             tpr_values,\n",
    "             color = \"darkorange\",\n",
    "             label = \"ROC Curve\")\n",
    "    \n",
    "    # Setting the baseline\n",
    "    plt.plot(np.linspace(0, 1, 500),\n",
    "             np.linspace(0, 1, 500),\n",
    "             color     = \"darkblue\",\n",
    "             label     = \"Baseline\"),\n",
    "    \n",
    "    # Setting model parameters\n",
    "    plt.title(title, fontsize = 18)\n",
    "    plt.ylabel(\"Sensitivity\", size = 16)\n",
    "    plt.xlabel(\"1 - Specificity\", size = 16)\n",
    "    plt.xticks(size = 14)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.legend(bbox_to_anchor = (1.04, 1), \n",
    "               loc            = \"upper left\",\n",
    "               fontsize       = 16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# The code was modified from code written by Matt Brems during our lesson on classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows for dataframes to be displayed side-by-side\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(html_str.replace('table', 'table style=\"display:inline\"'), raw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics scores\n",
    "\n",
    "balanced_accuracy = [balanced_accuracy_score(y_test, of_test_preds),\n",
    "                     balanced_accuracy_score(y_sq_test, sq_test_preds),\n",
    "                     balanced_accuracy_score(y_if_test, if_test_preds)]\n",
    "\n",
    "specificity       = [specificity(y_test, of_test_preds),\n",
    "                     specificity(y_sq_test, sq_test_preds),\n",
    "                     specificity(y_if_test, if_test_preds)]\n",
    "\n",
    "sensitivity       = [recall_score(y_test, of_test_preds),\n",
    "                     recall_score(y_sq_test, sq_test_preds),\n",
    "                     recall_score(y_if_test, if_test_preds)]\n",
    "\n",
    "f1_score          = [f1_score(y_test, of_test_preds),\n",
    "                     f1_score(y_sq_test, sq_test_preds),\n",
    "                     f1_score(y_if_test, if_test_preds)]\n",
    "\n",
    "rocauc_score      = [roc_auc_score(y_test, of_test_preds),\n",
    "                     roc_auc_score(y_sq_test, sq_test_preds),\n",
    "                     roc_auc_score(y_if_test, if_test_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lists into dataframes\n",
    "\n",
    "# A dataframe for the scores\n",
    "\n",
    "scores = pd.DataFrame(data    = [balanced_accuracy, specificity,\n",
    "                                 sensitivity, f1_score],\n",
    "                      columns = [\"Original Features\", \"Squared Features\", \n",
    "                                 \"Interaction Features\"],\n",
    "                      index   = [\"Balanced Accuracy\", \"Specificity\", \n",
    "                                 \"Sensitivity\", \"F1 Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the dataframe looks right\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(df     = scores*100,\n",
    "            column = \"Squared Features\",\n",
    "            label  = \"Squared Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the ROC-AUC score\n",
    "\n",
    "rocauc_score = round(roc_auc_score(y_sq_test, sq_test_preds), 5)\n",
    "\n",
    "# Plotting the ROC curve with the score in the title\n",
    "\n",
    "roc(model_prob = sq_model,\n",
    "    X_test     = X_sq_test,\n",
    "    y_test     = y_sq_test,\n",
    "    y_pred     = sq_test_preds,\n",
    "    title      = f\"ROC For Neural Network With A Score Of {rocauc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
